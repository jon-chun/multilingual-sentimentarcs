{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vFt5gu03r1rc","outputId":"4d904e04-0b76-4cf1-e991-aaa3b3b1fa48"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/linuxbrew/.linuxbrew/opt/python@3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"]}],"source":["import os\n","import json\n","import time\n","from torch import nn\n","from transformers import pipeline\n","from transformers import BertTokenizer\n","from transformers import BertForMaskedLM"]},{"cell_type":"markdown","metadata":{"id":"7FSOuO0tr1rl"},"source":["First, download and extract the [2018 Wikipedia dumps](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2735) in the data folder for the 15 XNLI languages :\n","\n","```bash\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/en.txt.gz -P data\n","gunzip data/en.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/fr.txt.gz -P data\n","gunzip data/fr.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/es.txt.gz -P data\n","gunzip data/es.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/de.txt.gz -P data\n","gunzip data/de.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/zh.txt.gz -P data\n","gunzip data/zh.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ar.txt.gz -P data\n","gunzip data/ar.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ru.txt.gz -P data\n","gunzip data/ru.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/vi.txt.gz -P data\n","gunzip data/vi.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/el.txt.gz -P data\n","gunzip data/el.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/bg.txt.gz -P data\n","gunzip data/bg.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/th.txt.gz -P data\n","gunzip data/th.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/tr.txt.gz -P data\n","gunzip data/tr.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/hi.txt.gz -P data\n","gunzip data/hi.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ur.txt.gz -P data\n","gunzip data/ur.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/sw.txt.gz -P data\n","gunzip data/sw.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/nl.txt.gz -P data\n","gunzip data/nl.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/uk.txt.gz -P data\n","gunzip data/uk.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ro.txt.gz -P data\n","gunzip data/ro.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/pt.txt.gz -P data\n","gunzip data/pt.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/it.txt.gz -P data\n","gunzip data/it.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/lt.txt.gz -P data\n","gunzip data/lt.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/no.txt.gz -P data\n","gunzip data/no.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/pl.txt.gz -P data\n","gunzip data/pl.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/da.txt.gz -P data\n","gunzip data/da.txt.gz\n","wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ja.txt.gz -P data\n","gunzip data/ja.txt.gz\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fp7HosLcr1rq"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnsXexa1r1rr","outputId":"ac678b21-bae6-43a6-a724-352234df7f5e"},"outputs":[{"data":{"text/plain":["119547"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["bert_vocab = list(tokenizer.vocab.keys())\n","len(bert_vocab)"]},{"cell_type":"markdown","metadata":{"id":"zWji1O_3r1rs"},"source":["# Select vocabularies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--VWzcPor1rt","outputId":"18ccb7b5-beaa-4859-ac92-a2b3fd1c9b1f"},"outputs":[{"data":{"text/plain":["25"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["languages = ['en', 'fr', 'es', 'de', 'zh', 'ar', 'ru', 'vi', 'el', 'bg', 'th', 'tr', 'hi', \n","              'ur', 'sw', 'nl', 'uk', 'ro', 'pt', 'it', 'lt', 'no', 'pl', 'da', 'ja']\n","len(languages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUXZqBYlr1ru"},"outputs":[],"source":["for lang in languages:\n","    num_lines = 0\n","    num_long_lines = 0\n","    path = 'data/'+lang+'.txt'\n","    with open(path) as infile:\n","        for line in infile:\n","            num_lines += 1\n","            if len(line)>5:\n","                num_long_lines += 1\n","    # compute frequencies\n","    lang_tokens = dict()\n","    lang_tokens_unique = dict()\n","    t0 = time.time()\n","    with open(path) as infile:\n","        for line in infile:\n","            if len(line)>5:\n","                tokens = tokenizer.tokenize(line)\n","                for token in tokens:\n","                    if token not in lang_tokens:\n","                        lang_tokens[token] = 1\n","                    else:\n","                        lang_tokens[token] += 1\n","                for token in list(set(tokens)):\n","                    if token not in lang_tokens_unique:\n","                        lang_tokens_unique[token] = 1\n","                    else:\n","                        lang_tokens_unique[token] += 1\n","    # save frequencies\n","    with open('tokens_freqs/'+lang+'_freqs.json', 'w') as outfile:\n","        json.dump(lang_tokens, outfile)\n","    seuil = int(num_long_lines*0.005/100)\n","    num_selected_tokens = 0\n","    with open('selected_tokens/selected_'+lang+'_tokens.txt', 'w') as output:\n","        for tok in lang_tokens_unique:\n","            if lang_tokens_unique[tok] >= seuil:\n","                output.write(tok+'\\n')\n","                num_selected_tokens += 1"]},{"cell_type":"markdown","metadata":{"id":"xmB-Z91ur1rw"},"source":["## Load all vocabs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdeNnJs-r1rx","outputId":"015a462b-f29e-4e64-ac7b-2bb120a641cb"},"outputs":[{"data":{"text/plain":["25"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["langs = dict()\n","\n","for l in languages:\n","    with open('selected_tokens/selected_'+l+'_tokens.txt') as file:\n","        langs[l] = file.read().splitlines()\n","len(langs)"]},{"cell_type":"markdown","metadata":{"id":"4XhttTBJr1ry"},"source":["## Choosing vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoWjFdWzr1r1","outputId":"4668f562-700f-4bc3-a9f5-2c99e29cc407"},"outputs":[{"data":{"text/plain":["84972"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["all_selected_tokens = []\n","for k in langs.keys():\n","    all_selected_tokens.extend(langs[k])\n","selected_tokens = list(set(all_selected_tokens))\n","len(selected_tokens)"]},{"cell_type":"markdown","metadata":{"id":"ChruM4zer1r1"},"source":["## Resize token embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiQnH3Jhr1r2","outputId":"1069fe8d-9be7-46ca-8709-088c9491ced4"},"outputs":[{"data":{"text/plain":["84985"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["TOKENS_TO_KEEP = ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]','[unused1]','[unused2]','[unused3]',\n","                  '[unused4]','[unused5]', '[unused6]','[unused7]','[unused8]','[unused9]']\n","\n","for tok in TOKENS_TO_KEEP:\n","    if tok not in selected_tokens:\n","        selected_tokens.append(tok)\n","\n","len(selected_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZhQwuvHr1r2"},"outputs":[],"source":["def select_embeddings(model, old_vocab, new_vocab, model_name='new_model'):\n","    \n","    # Get old embeddings from model\n","    old_embeddings = model.get_input_embeddings()\n","    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n","    \n","    if old_num_tokens != len(old_vocab):\n","        print('len(old_vocab) != len(model.old_embeddings)')\n","        return old_embeddings\n","    \n","    new_num_tokens = len(new_vocab)\n","    if new_vocab is None:\n","        print('nothing to copy')\n","        return old_embeddings\n","    \n","    # Build new embeddings\n","    new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n","    new_embeddings.to(old_embeddings.weight.device)\n","    \n","    # Copy weights\n","    i = 0\n","    j = 0\n","    vocab = []\n","    for token in old_vocab:\n","        if token in new_vocab:\n","            vocab.append(token)\n","            new_embeddings.weight.data[i, :] = old_embeddings.weight.data[j, :]\n","            i += 1\n","        j += 1\n","    \n","    model.set_input_embeddings(new_embeddings)\n","    \n","    # Update base model and current model config\n","    model.config.vocab_size = new_num_tokens\n","    model.vocab_size = new_num_tokens\n","\n","    # Tie weights\n","    model.tie_weights()\n","    \n","    # Save new model\n","    model.save_pretrained(model_name)\n","    print(model_name, \" - \", \" num_parameters : \", model.num_parameters())\n","    print(model_name, \" - \", \" num_tokens : \", len(vocab))\n","    \n","    # Save vocab\n","    fw = open(os.path.join(model_name, 'vocab.txt'), 'w')\n","    for token in vocab:\n","        fw.write(token+'\\n')\n","    fw.close()\n","    \n","    # Save tokenizer config\n","    fw = open(os.path.join(model_name, 'tokenizer_config.json'), 'w')\n","    json.dump({\"do_lower_case\": False, \"model_max_length\": 512}, fw)\n","    fw.close()\n","    \n","    return new_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHMA1QpVr1r3","outputId":"9e94fa10-a46e-451d-c758-5cf5b032178a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["177974523"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","model_cased.num_parameters()"]},{"cell_type":"markdown","metadata":{"id":"2z4uw3knr1r3"},"source":["# Generating models"]},{"cell_type":"markdown","metadata":{"id":"g14doIyyr1r4"},"source":["## Generating 25langs model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFaGY0RCr1r4","outputId":"ef36694e-b767-4cd3-ea23-e3acd2eb0f2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["new-models/bert-base-25lang-cased  -   num_parameters :  151396345\n","new-models/bert-base-25lang-cased  -   num_tokens :  84985\n","449.4667663574219\n"]},{"data":{"text/plain":["Embedding(84985, 768)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, selected_tokens, 'new-models/bert-base-25lang-cased')\n","print(time.time()-t)\n","new_embs"]},{"cell_type":"markdown","metadata":{"id":"fLfVkRSRr1r5"},"source":["## Generating 5langs models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwxvfZitr1r5","outputId":"31eff00e-0697-4be3-9814-763e03c9a00b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-es-de-zh-cased  -   num_parameters :  125126536\n","new-models/bert-base-en-fr-es-de-zh-cased  -   num_tokens :  50824\n","158.7407102584839\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['es']+\n","                                                               langs['de']+langs['zh']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-es-de-zh-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-TuMbRhr1r6","outputId":"c1cd0422-2940-4c89-ff1d-9bc812442c19"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-nl-ru-ar-cased  -   num_parameters :  123720035\n","new-models/bert-base-en-fr-nl-ru-ar-cased  -   num_tokens :  48995\n","143.9530508518219\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['nl']+\n","                                                               langs['ru']+langs['ar']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-nl-ru-ar-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BKrHor3r1r6","outputId":"a3fa72fb-e0f9-4350-c601-f52926ccef3d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-uk-el-ro-cased  -   num_parameters :  120014224\n","new-models/bert-base-en-fr-uk-el-ro-cased  -   num_tokens :  44176\n","120.72881722450256\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['uk']+\n","                                                               langs['el']+langs['ro']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-uk-el-ro-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"loDHO1Z3r1r7","outputId":"c7a7ac0e-e37f-41ce-ae96-a19d50e4548d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-es-pt-it-cased  -   num_parameters :  119231382\n","new-models/bert-base-en-fr-es-pt-it-cased  -   num_tokens :  43158\n","157.18285059928894\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['es']+\n","                                                               langs['pt']+langs['it']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-es-pt-it-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StClSuA4r1r7","outputId":"5af23d9d-d5c5-4cc1-f107-a6c69316f4ff"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-lt-no-pl-cased  -   num_parameters :  118497756\n","new-models/bert-base-en-fr-lt-no-pl-cased  -   num_tokens :  42204\n","104.5706377029419\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['lt']+\n","                                                               langs['no']+langs['pl']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-lt-no-pl-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziQGaoTPr1r8","outputId":"3ca2b5f2-8b96-4f90-e893-d234ad47594d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-zh-ja-vi-cased  -   num_parameters :  119745074\n","new-models/bert-base-en-fr-zh-ja-vi-cased  -   num_tokens :  43826\n","131.54477763175964\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['zh']+\n","                                                               langs['ja']+langs['vi']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-zh-ja-vi-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4eroL11Kr1r8","outputId":"6613a97a-c5e6-4d16-b40c-b8718c084d6f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-de-no-da-cased  -   num_parameters :  118117870\n","new-models/bert-base-en-fr-de-no-da-cased  -   num_tokens :  41710\n","124.39375185966492\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['de']+\n","                                                               langs['no']+langs['da']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-de-no-da-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzrmLuXdr1r8","outputId":"15341825-2ed1-4c15-f969-a5c0f1c149fa"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-da-ja-vi-cased  -   num_parameters :  119753533\n","new-models/bert-base-en-fr-da-ja-vi-cased  -   num_tokens :  43837\n","106.04906463623047\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['da']+\n","                                                               langs['ja']+langs['vi']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-da-ja-vi-cased')\n","print(time.time()-t)"]},{"cell_type":"markdown","metadata":{"id":"eFopPzkIr1r9"},"source":["## Generating trilingual models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvbUNU8nr1r9","outputId":"9d5b6ae7-1949-402b-9fb0-aeac89b90804"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-es-cased  -   num_parameters :  116154613\n","new-models/bert-base-en-fr-es-cased  -   num_tokens :  39157\n","109.01875948905945\n"]}],"source":["model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['es']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-es-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nhb8JLOhr1r9","outputId":"febd87a1-e098-4d61-bbe3-59dbeb75cb14"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-es-it-cased  -   num_parameters :  115747812\n","new-models/bert-base-en-es-it-cased  -   num_tokens :  38628\n","107.14139437675476\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+langs['it']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-es-it-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ba52sSKCr1r-","outputId":"b6fb8bc8-1636-4fc3-bb88-29262a0337c8"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-es-pt-cased  -   num_parameters :  114788100\n","new-models/bert-base-en-es-pt-cased  -   num_tokens :  37380\n","110.02508020401001\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+langs['pt']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-es-pt-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7ezc2N1r1r-","outputId":"5b9a6220-fb5e-453e-912c-f06cf6af4f34"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-de-cased  -   num_parameters :  116043877\n","new-models/bert-base-en-fr-de-cased  -   num_tokens :  39013\n","110.83046197891235\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['de']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-de-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdYwcZk4r1r-","outputId":"1209e7ab-423d-4fa0-d3a3-dbf30fb2d5b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-it-cased  -   num_parameters :  114829626\n","new-models/bert-base-en-fr-it-cased  -   num_tokens :  37434\n","102.47859787940979\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['it']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-it-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVUNlqilr1r_","outputId":"dcb65ca2-0fcb-4659-8678-811048c6e1da"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-zh-cased  -   num_parameters :  116735208\n","new-models/bert-base-en-fr-zh-cased  -   num_tokens :  39912\n","102.47170114517212\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['zh']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-zh-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LKIfinXr1r_","outputId":"9ce95135-aeb1-4796-b4fe-89ccca087804"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-es-zh-cased  -   num_parameters :  118330114\n","new-models/bert-base-en-es-zh-cased  -   num_tokens :  41986\n","121.32015228271484\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+langs['zh']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-es-zh-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9xhC__Nr1r_","outputId":"5f8223f4-a923-44bd-c4f1-3a2a734fbbeb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-fr-ar-cased  -   num_parameters :  114258259\n","new-models/bert-base-en-fr-ar-cased  -   num_tokens :  36691\n","118.46728754043579\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['ar']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-fr-ar-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7s4UE9Vkr1sA","outputId":"2a1c29d5-b3f3-47cb-a71c-c67e1e3f4c6b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-el-ru-cased  -   num_parameters :  116167686\n","new-models/bert-base-en-el-ru-cased  -   num_tokens :  39174\n","111.97321152687073\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['el']+langs['ru']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-el-ru-cased')\n","print(time.time()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vf6RkqvNr1sA","outputId":"14e49f2c-04a0-4fb0-f4fd-0cd472045913"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-zh-hi-cased  -   num_parameters :  114350539\n","new-models/bert-base-en-zh-hi-cased  -   num_tokens :  36811\n","84.94034099578857\n"]}],"source":["del model_cased\n","model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","t = time.time()\n","new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['zh']+langs['hi']+TOKENS_TO_KEEP)),\n","                             'new-models/bert-base-en-zh-hi-cased')\n","print(time.time()-t)"]},{"cell_type":"markdown","metadata":{"id":"qecdmOATr1sA"},"source":["## Generating bilingual models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQPbcr_Yr1sA","outputId":"ba99669e-761f-428f-c952-d6211e5238ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-nl-cased  -   num_parameters :  111575987\n","new-models/bert-base-en-nl-cased  -   num_tokens :  33203\n","106.0707049369812\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-uk-cased  -   num_parameters :  113307775\n","new-models/bert-base-en-uk-cased  -   num_tokens :  35455\n","99.78673195838928\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-ro-cased  -   num_parameters :  110857741\n","new-models/bert-base-en-ro-cased  -   num_tokens :  32269\n","92.63383269309998\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-pt-cased  -   num_parameters :  112438805\n","new-models/bert-base-en-pt-cased  -   num_tokens :  34325\n","98.68715047836304\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-it-cased  -   num_parameters :  112105059\n","new-models/bert-base-en-it-cased  -   num_tokens :  33891\n","96.94106602668762\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-lt-cased  -   num_parameters :  110510153\n","new-models/bert-base-en-lt-cased  -   num_tokens :  31817\n","91.33998203277588\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-no-cased  -   num_parameters :  111474479\n","new-models/bert-base-en-no-cased  -   num_tokens :  33071\n","94.8031907081604\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-pl-cased  -   num_parameters :  112090448\n","new-models/bert-base-en-pl-cased  -   num_tokens :  33872\n","97.298259973526\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-da-cased  -   num_parameters :  111213019\n","new-models/bert-base-en-da-cased  -   num_tokens :  32731\n","94.70722985267639\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-en-ja-cased  -   num_parameters :  111672112\n","new-models/bert-base-en-ja-cased  -   num_tokens :  33328\n","105.59650444984436\n","\n"]}],"source":["for lang in list(langs.keys())[-10:]:\n","    model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","    t = time.time()\n","    new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs[lang]+TOKENS_TO_KEEP)), \n","                                 'new-models/bert-base-en-'+lang+'-cased')\n","    del model_cased\n","    print(time.time()-t)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"Q6CECb3-r1sA"},"source":["## Generating monolingual models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlF-Pm7Or1sB","outputId":"ef6ee07b-d28e-4636-ed45-be010262478a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-nl-cased  -   num_parameters :  104251262\n","new-models/bert-base-nl-cased  -   num_tokens :  23678\n","81.82937741279602\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-uk-cased  -   num_parameters :  95125539\n","new-models/bert-base-uk-cased  -   num_tokens :  11811\n","36.54637026786804\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-ro-cased  -   num_parameters :  102620982\n","new-models/bert-base-ro-cased  -   num_tokens :  21558\n","72.35977983474731\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-pt-cased  -   num_parameters :  105267880\n","new-models/bert-base-pt-cased  -   num_tokens :  25000\n","76.86364984512329\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-it-cased  -   num_parameters :  105649304\n","new-models/bert-base-it-cased  -   num_tokens :  25496\n","81.77199244499207\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-lt-cased  -   num_parameters :  98382254\n","new-models/bert-base-lt-cased  -   num_tokens :  16046\n","48.86039853096008\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-no-cased  -   num_parameters :  104036711\n","new-models/bert-base-no-cased  -   num_tokens :  23399\n","80.93481659889221\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-pl-cased  -   num_parameters :  103266173\n","new-models/bert-base-pl-cased  -   num_tokens :  22397\n","83.10979652404785\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-da-cased  -   num_parameters :  103845230\n","new-models/bert-base-da-cased  -   num_tokens :  23150\n","82.06179618835449\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["new-models/bert-base-ja-cased  -   num_parameters :  93342228\n","new-models/bert-base-ja-cased  -   num_tokens :  9492\n","32.165956258773804\n","\n"]}],"source":["for lang in list(langs.keys())[-10:]:\n","    model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","    t = time.time()\n","    new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs[lang]+TOKENS_TO_KEEP)), \n","                                 'new-models/bert-base-'+lang+'-cased')\n","    del model_cased\n","    print(time.time()-t)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"gDM3Nmdcr1sB"},"source":["# Compare original and new models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSFyF0Ezr1sB","outputId":"3bba4cb7-5f30-4741-eb59-ee54963f1012"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["177974523"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# original model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","model.num_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"b2jaRTYor1sB","outputId":"57ad7c22-9c5a-4f04-add9-c8a6d52fd989"},"outputs":[{"data":{"text/plain":["114258259"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# new model\n","tokenizer_cust = BertTokenizer.from_pretrained('new-models/bert-base-en-fr-ar-cased')\n","model_cust = BertForMaskedLM.from_pretrained('new-models/bert-base-en-fr-ar-cased')\n","model_cust.num_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toPUSH1hr1sC","outputId":"000d0de1-9cf9-4efb-c8b8-bc70e313a7b7"},"outputs":[{"data":{"text/plain":["36691"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(tokenizer_cust.get_vocab())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbhVcxyFr1sC","outputId":"90c2ffa3-b50a-498d-e296-76f9985c7871"},"outputs":[{"data":{"text/plain":["Embedding(36691, 768, padding_idx=0)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model_cust.get_input_embeddings()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6iRiPNEr1sC"},"outputs":[],"source":["text = \"I love NLP\"\n","encoded_input = tokenizer(text, return_tensors='pt')\n","output_original = model(**encoded_input)\n","encoded_input_cust = tokenizer_cust(text, return_tensors='pt')\n","output_cust = model_cust(**encoded_input_cust)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywKsJj2hr1sC","outputId":"8e0a10a1-b3ba-44a5-ed51-50b722e66ee8"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[  101,   146, 16138, 81130, 11127,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["encoded_input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAqi0Crtr1sD","outputId":"71929f92-8c95-4d4b-f4c6-797dd443ccba"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[   11,    54,  3953, 28486,  1043,    12]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["encoded_input_cust"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZPLW0xZr1sD","outputId":"7efbe9c6-4f49-4142-9344-4a06a57603e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["6 6\n"]}],"source":["print(len(output_original[0][0]), len(output_cust[0][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dopO6-okr1sE","outputId":"7a2d5ffd-7628-49b6-fe52-4bf1a7380171"},"outputs":[{"data":{"text/plain":["tensor([[-8.5026, -8.4598, -8.5441,  ..., -8.4676, -8.3309, -8.4011],\n","        [-7.3152, -7.4170, -7.2602,  ..., -6.7312, -7.1424, -7.0654],\n","        [-8.5618, -9.1271, -7.8516,  ..., -8.3914, -7.0207, -8.6194],\n","        [-6.9560, -6.5613, -6.1853,  ..., -6.7822, -6.2483, -6.3508],\n","        [-8.5762, -7.8520, -6.7847,  ..., -8.3420, -6.3392, -7.7183],\n","        [-7.3157, -7.2129, -6.7588,  ..., -6.8620, -6.5756, -8.0586]],\n","       grad_fn=<SelectBackward>)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["output_original[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0T_xBVrcr1sE","outputId":"fc9f04d6-aab8-4668-d053-f8925aea9492"},"outputs":[{"data":{"text/plain":["tensor([[-8.5026, -8.4598, -8.5441,  ..., -4.2134, -2.0971, -1.1937],\n","        [-7.3152, -7.4170, -7.2602,  ..., -5.6871, -5.9795, -3.7613],\n","        [-8.5618, -9.1271, -7.8516,  ..., -6.0970, -6.2893,  0.0856],\n","        [-6.9560, -6.5613, -6.1853,  ..., -4.5743, -2.0049, -0.7851],\n","        [-8.5762, -7.8520, -6.7847,  ..., -3.6335, -5.0034, -0.6123],\n","        [-7.3157, -7.2129, -6.7588,  ..., -5.2001, -5.3873,  1.1282]],\n","       grad_fn=<SelectBackward>)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["output_cust[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_gEXf4Dr1sF","outputId":"43df162c-c3e3-4c46-a74f-c7715a4d6e34"},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS]\n","[-8.502596 -8.459798 -8.544106 -8.420239 -8.55526  -8.383109]\n","[-8.502596 -8.459798 -8.544106 -8.420239 -8.55526  -8.383109]\n","\n","I\n","[-7.3151646 -7.416972  -7.260161  -7.000843  -7.0258822 -6.568579 ]\n","[-7.3151646 -7.416972  -7.260161  -7.000843  -7.0258822 -6.568579 ]\n","\n","love\n","[-8.561801  -9.127073  -7.8515744 -8.405504  -8.349455  -8.195387 ]\n","[-8.561801  -9.127073  -7.8515744 -8.405504  -8.349455  -8.195387 ]\n","\n","NL\n","[-6.9560323 -6.5612655 -6.185295  -5.8626823 -6.8318934 -6.2828846]\n","[-6.9560323 -6.5612655 -6.185295  -5.8626823 -6.8318934 -6.2828846]\n","\n","##P\n","[-8.576168  -7.852015  -6.784669  -7.650504  -7.808926  -7.4190974]\n","[-8.576168  -7.852015  -6.784669  -7.650504  -7.808926  -7.4190974]\n","\n","[SEP]\n","[-7.315665  -7.2128882 -6.75876   -6.995212  -7.240693  -7.080781 ]\n","[-7.315665  -7.2128882 -6.75876   -6.995212  -7.240693  -7.080781 ]\n","\n"]}],"source":["i = 0\n","for input_id in encoded_input['input_ids'][0]:\n","    print(tokenizer.convert_ids_to_tokens(int(input_id)))\n","    print(output_original[0][0][i].detach().numpy()[:6])\n","    print(output_cust[0][0][i].detach().numpy()[:6])\n","    print()\n","    i+=1"]},{"cell_type":"markdown","metadata":{"id":"zfA8x_djr1sF"},"source":["## Tests on MLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AD03IXTYr1sG","outputId":"209facbd-e1fe-49bb-9929-d201adc1ea0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["capital 0.6365790963172913\n","city 0.08376165479421616\n","City 0.034411922097206116\n","port 0.02745007537305355\n","centre 0.012592659331858158\n"]}],"source":["## declare task ##\n","pipe = pipeline(task=\"fill-mask\", model=model, tokenizer=tokenizer)\n","\n","## example ##\n","input_  = 'Paris is the [MASK] of France.'\n","\n","output_ = pipe(input_)\n","for i in range(len(output_)):\n","    print(output_[i]['token_str'], output_[i]['score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlhbqs5ur1sG","outputId":"b5223b94-1058-4836-a83c-29e296e096ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["capital 0.6015416979789734\n","city 0.07779796421527863\n","City 0.035266101360321045\n","port 0.028833329677581787\n","centre 0.014866690151393414\n"]}],"source":["## declare task ##\n","pipe = pipeline(task=\"fill-mask\", model=model_cust, tokenizer=tokenizer_cust)\n","\n","## example ##\n","input_  = 'Paris is the [MASK] of France.'\n","\n","output_ = pipe(input_)\n","for i in range(len(output_)):\n","    print(output_[i]['token_str'], output_[i]['score'])"]},{"cell_type":"markdown","metadata":{"id":"IUpHztKsr1sG"},"source":["# Convert all models to TF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJ91uCVDr1sG"},"outputs":[],"source":["from transformers import TFBertForMaskedLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56wp_IO1r1sH"},"outputs":[],"source":["for model_name in os.listdir('new-models'):\n","    tf_model = TFBertForMaskedLM.from_pretrained(\"new-models/\"+model_name, from_pt=True)\n","    tf_model.save_pretrained(\"new-models/\"+model_name)\n","    del tf_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFtnv6yPr1sH"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"name":"transformer_distilled_mBERT_vocabularies_20220212.ipynb","provenance":[{"file_id":"https://github.com/Geotrend-research/smaller-transformers/blob/main/notebooks/select_mBERT_vocabularies.ipynb","timestamp":1644732312668}],"collapsed_sections":["g14doIyyr1r4","fLfVkRSRr1r5","eFopPzkIr1r9","qecdmOATr1sA","Q6CECb3-r1sA","zfA8x_djr1sF"]}},"nbformat":4,"nbformat_minor":0}