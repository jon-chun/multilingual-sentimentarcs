{"cells":[{"cell_type":"markdown","metadata":{"id":"bZfth4OEexw8"},"source":["# I - Installing the package"]},{"cell_type":"markdown","metadata":{"id":"L_ZwxpePexw_"},"source":["It's as simple as : \n","\n","1/ Open terminal\n","\n","2/ Run 'pip install spanish_sentiment_analysis'"]},{"cell_type":"code","source":["!pip install spanish_sentiment_analysis"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kdfP8dge4XI","executionInfo":{"status":"ok","timestamp":1670918170522,"user_tz":300,"elapsed":12071,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"9a2af85e-b7df-410c-d9d0-baebe2e62665"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spanish_sentiment_analysis\n","  Downloading spanish_sentiment_analysis-1.0.0-py3-none-any.whl (15.8 MB)\n","\u001b[K     |████████████████████████████████| 15.8 MB 303 kB/s \n","\u001b[?25hCollecting marisa-trie\n","  Downloading marisa_trie-0.7.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 41.9 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from spanish_sentiment_analysis) (1.7.3)\n","Collecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from spanish_sentiment_analysis) (1.21.6)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from spanish_sentiment_analysis) (3.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from marisa-trie->spanish_sentiment_analysis) (57.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->spanish_sentiment_analysis) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->spanish_sentiment_analysis) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->spanish_sentiment_analysis) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->spanish_sentiment_analysis) (1.2.0)\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=96fa923670d53d72d2a9ca2f74f30add8c9c9ce1c28d878786128029f0129971\n","  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n","Successfully built sklearn\n","Installing collected packages: sklearn, marisa-trie, spanish-sentiment-analysis\n","Successfully installed marisa-trie-0.7.8 sklearn-0.0.post1 spanish-sentiment-analysis-1.0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"SX4zg-dAexxA"},"source":["# II - Usage of the package"]},{"cell_type":"markdown","metadata":{"id":"Rko_nGdDexxA"},"source":["Import the module, might takes a few seconds"]},{"cell_type":"code","source":["!pip install joblib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4bJ5vydfEZc","executionInfo":{"status":"ok","timestamp":1670918202626,"user_tz":300,"elapsed":3809,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"e688b6bb-aad3-451b-8198-d59b72482c3a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (1.2.0)\n"]}]},{"cell_type":"code","source":["import joblib"],"metadata":{"id":"ncMW7GnnfYsw","executionInfo":{"status":"ok","timestamp":1670918280838,"user_tz":300,"elapsed":3,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"id":"k19FQolnexxB","colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"status":"error","timestamp":1670918281924,"user_tz":300,"elapsed":106,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"0996478a-a45d-400a-a7b3-5ee4e791afd1"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-d5cb0f346320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/classifier/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msentimentPipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscript_global\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/classifier/sentimentPipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/usr/local/lib/python3.8/dist-packages/sklearn/externals/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from classifier import *"]},{"cell_type":"markdown","metadata":{"id":"L0guAotWexxC"},"source":["Create a classifier from the class SentimentClassifier.\n","\n","Might takes a few seconds two, because it has to load the model in model/sentiment_pipeline.pkl"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":true,"id":"rO3TPHRnexxC","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"error","timestamp":1670918519460,"user_tz":300,"elapsed":142,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"9c1fd5ba-1370-4fea-9655-097e82272a59"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-95e847189742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'SentimentClassifier' is not defined"]}],"source":["clf = SentimentClassifier()"]},{"cell_type":"markdown","metadata":{"id":"7AWFcJF6exxC"},"source":["You can now get the prediction of a text, using the predict function of the classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MS8NpN9exxD","outputId":"6a9791d9-ae46-4570-9a51-fdefd4e289c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Esta muy buena esa pelicula ==> 0.93136\n"]}],"source":["x = \"Esta muy buena esa pelicula\"\n","\n","print(x + ' ==> %.5f' % clf.predict(x))"]},{"cell_type":"markdown","metadata":{"id":"zYw8lFUjexxE"},"source":["Model is trained so that you should be able to write with repeated letters (commun with the spanish language),\n","\n","uppercased or lowercased letters, with whatsoever punctuation, and various internet/sms kind of abbrevations/shortcuts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkLAdTElexxE","outputId":"ae5aaa98-758a-4efb-bae8-7dc372efc8e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Son malaaaaa onda esos vecinossss ==> 0.22325\n","el BIFE estuvo EXCELENTE ==> 0.91101\n","!!!! Quien te pensas que sos ??Ahora andate!!y no te quiero ver mas..! ==> 0.08893\n","K askoooo ==> 0.01420\n","te la voy a meter x el culo ==> 0.04589\n"]}],"source":["X = [\"Son malaaaaa onda esos vecinossss\", 'el BIFE estuvo EXCELENTE',\n","     '!!!! Quien te pensas que sos ??Ahora andate!!y no te quiero ver mas..!',\n","     'K askoooo', 'te la voy a meter x el culo']\n","\n","for e in X:\n","    print(e + ' ==> %.5f' % clf.predict(e))"]},{"cell_type":"markdown","metadata":{"id":"9Pb4b541exxE"},"source":["The model is not sensitive to accents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9qDL2RiexxF","outputId":"40f9b29a-e2a3-4bdb-8119-ea1a6e51b8d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Me comí una pared en la cabeza, duele ==> 0.16427\n","me comi una pared en la cabeza  duele ==> 0.16427\n"]}],"source":["X = [\"Me comí una pared en la cabeza, duele\", \"me comi una pared en la cabeza  duele\"]\n","for e in X:\n","    print(e + ' ==> %.5f' % clf.predict(e))"]},{"cell_type":"markdown","metadata":{"id":"nIvRSePAexxF"},"source":["The model won't pay attention to town and countries name. This is important because otherwise it would have a huge bias since an important part of the comments are extracted from tripadvisor and thus town/country words might have a good/bad score\n","instead of neutral."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JdZlNV_lexxF","outputId":"7d826277-36c1-43d7-c989-1d35c5568a66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Paris es una mierda ==> 0.04911\n","Buenos aires es una mierda ==> 0.04911\n","Nunca iré en quilmes me dijeron que la zona sur es peligrosa ==> 0.25738\n","Paraguay es un pais maravilloso ==> 0.93276\n","Espana es un pais maravilloso ==> 0.93276\n","Chile es una poronga ==> 0.15804\n","Los de Francia tienen buen olor ==> 0.58273\n","Los de Argentina tienen mal olor ==> 0.20130\n"]}],"source":["X = ['Paris es una mierda', 'Buenos aires es una mierda', 'Nunca iré en quilmes me dijeron que la zona sur es peligrosa',\n","     'Paraguay es un pais maravilloso','Espana es un pais maravilloso', 'Chile es una poronga',\n","     'Los de Francia tienen buen olor', 'Los de Argentina tienen mal olor']\n","\n","for e in X:\n","    print(e + ' ==> %.5f' % clf.predict(e))"]},{"cell_type":"markdown","metadata":{"id":"kWNKZpt9exxG"},"source":["Also, a list of around 100 most common verbs is used to factorize verbs amond comments. For example :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpFGHq5LexxG","outputId":"7c78232e-cfaf-4077-d174-09ac3056d484"},"outputs":[{"name":"stdout","output_type":"stream","text":["Soy un grande ==> 0.79150\n","Es un grande ==> 0.79150\n","Eres un grande ==> 0.79150\n"]}],"source":["X = ['Soy un grande', 'Es un grande', 'Eres un grande']\n","\n","for e in X:\n","    print(e + ' ==> %.5f' % clf.predict(e))"]},{"cell_type":"markdown","metadata":{"id":"bz7OPPIDexxG"},"source":["A custom choice is implemented before the prediction : If the sentence has a 'pero', two scores are calculated on the parts\n","that come before and after the 'pero', and an adaptative barycenter of the two scores is calculted so that the final score\n","will get the same polarity as the second part of the sentence, but taking into account the the score of the first part.\n","\n","This allows to catch nuances in the sentence, if it starts saying something positive but concludes saying something negative\n","the overall polarity should be negative, but contrasted by how positive was the first part. Examples :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbsnYygSexxG","outputId":"7e5953c8-aa59-489e-92ce-3c3de931bb72"},"outputs":[{"name":"stdout","output_type":"stream","text":["No me gusto la pelicula ==> 0.25541\n","Los actores son buenos, pero no me gusto la pelicula igual ==> 0.49656\n","Me gusto la pelicula ==> 0.74060\n","El actor es malo, pero me gusto la pelicula igual ==> 0.52839\n"]}],"source":["X = ['No me gusto la pelicula',\n","    'Los actores son buenos, pero no me gusto la pelicula igual',\n","     'Me gusto la pelicula',\n","     'El actor es malo, pero me gusto la pelicula igual']\n","for e in X:\n","    print(e + ' ==> %.5f' % clf.predict(e))"]},{"cell_type":"markdown","metadata":{"id":"3f21eJZwexxH"},"source":["A similar thing is done to process comments that present amplification terms 'muy', 're', etc, so that the overall polarity\n","is the polarity of the verb or adjective concerned by the amplification term"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9reOwlUiexxH","outputId":"7c35ec75-2c11-400c-9dcd-3481f641ff26"},"outputs":[{"name":"stdout","output_type":"stream","text":["Esa tienda es recomendable ==> 0.56918\n","Esta tienda es muy recomendable ==> 0.73411\n"]}],"source":["X = ['Esa tienda es recomendable', 'Esta tienda es muy recomendable']\n","\n","for e in X:\n","    print(e + ' ==> %.5f' % clf.predict(e))"]},{"cell_type":"markdown","metadata":{"id":"pZ2aCh0wexxH"},"source":["The model is also trained to understand negation, no, sin, nunca, jamas, ni"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLXi0ohzexxH","outputId":"beead9a1-fcbf-405e-97c6-494b52afeca8"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \n","                   DIRECTOS                                                           NEGATIVOS\n"," \n","Me gusto la pelicula ==>  0.74060                          no me gusto la pelicula ==>  0.25541\n","Es gente buena ==>  0.85803                                no es gente buena ==>  0.33116\n","Estuvo interesante hablar con usted ==>  0.72379           no me interesa hablar con vos ==>  0.06870\n","lo recomiendo ==>  0.73870                                 no lo recomiendo ==>  0.04422\n","me aburri un monton ==>  0.00940                           no te podes aburrir con esa serie ==>  0.82862\n","Me quede muy contento! ==>  0.91250                        nunca estas contento ==>  0.23762\n","Saludar con una sonrisa! ==>  0.61671                      siempre saluda sin sonreir ==>  0.48202\n","El saluda a la gente cuando se cruza ==>  0.70492          y a mi, ni me saludo ==>  0.07436\n","Que rica que estuvo la comida! ==>  0.83106                la comida jamas es rica ==>  0.26195\n"]}],"source":["X = ['Me gusto la pelicula' ,'Es gente buena',\n","     'Estuvo interesante hablar con usted',\n","     'lo recomiendo', 'me aburri un monton',\n","     'Me quede muy contento!','Saludar con una sonrisa!',\n","     'El saluda a la gente cuando se cruza','Que rica que estuvo la comida!']     \n","Y = ['no me gusto la pelicula', 'no es gente buena',\n","     'no me interesa hablar con vos',\n","     'no lo recomiendo', 'no te podes aburrir con esa serie',\n","    'nunca estas contento', 'siempre saluda sin sonreir',\n","     'y a mi, ni me saludo', 'la comida jamas es rica']\n","\n","\n","print(' ')\n","print(' '.join([''] * 20) + 'DIRECTOS' + ' '.join([''] * 60) + 'NEGATIVOS')\n","print(' ')\n","for e,f in list(zip(X,Y)):\n","    toPrint1 = e + ' ==>  %.5f' % clf.predict(e)\n","    toPrint2 = f + ' ==>  %.5f' % clf.predict(f)\n","    print(toPrint1 + ' '.join([''] * (60-len(toPrint1))) + toPrint2)"]},{"cell_type":"markdown","metadata":{"id":"1SLXjy0HexxI"},"source":["The model is trained with many argentine expressions, so it should understand 'la puteada portena'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7d00hJ0exxI","outputId":"8f61b1ed-66ad-4b8b-e5ee-5a1762e2378b"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \n","                   MALOS                                                           BUENOS\n"," \n","Andate a cagar hdp ==>  0.01768                         Que piola che ==>  0.61599\n","Chupenme la ==>  0.00517                                Ese viaje es barbaro ==>  0.90828\n","Pelotudos del orto ==>  0.00761                         Un chico muy copado!! ==>  0.83341\n","Chorros de mierda ==>  0.00834                          Buenisima la atencion ==>  0.95254\n","Vos sos gil o que ==>  0.07238                          La posta que estuvo espectacular la comida ==>  0.86124\n","tarado callate ==>  0.06949                             Es una ciudad muy buena onda ==>  0.96940\n","Me cago en la concha de tu hermana ==>  0.00637         No te podes aburrir con ellos! ==>  0.72462\n","La puta que lo pario ==>  0.01347                       Excelente, nada que decir ==>  0.71844\n","la concha de la lora, te voy a matar ==>  0.00718       Reeeeee recomiendo esa pelicula ==>  0.73870\n","ni enpedo votaré para amalia granata ==>  0.01525       Todo estuvo perfecto ==>  0.92008\n","esos chicos son tarados completos ==>  0.54273          El bife de lomo riquisimo ==>  0.98693\n","la recalcada puta que lo pario ==>  0.00068             Me enamoré de esos paisajes ==>  0.95135\n","hijo de mil putas ==>  0.02275                          Vamooo Argentina!! ==>  0.88436\n","es una porkeria ==>  0.01987                            Me quedé muy contento ==>  0.91250\n","que quilombo ==>  0.19475                               El precio es honesto ==>  0.77174\n","Una verdadera choteada ==>  0.06227                     Ese hotel tenia todo para satisfechar ==>  0.85473\n","Vos decis pelotudeces todo el tiempo ==>  0.10851       Muy bomba la pasamos ==>  0.89354\n","vinimos al re pedo ==>  0.23703                         Los apoyo con todo mi ser ==>  0.75060\n","ese pibe es muy cagon ==>  0.06757                      confio totalmente en esa persona ==>  0.88127\n","dale no seas rata ==>  0.01513                          Es el mejor dia de mi vida creo ==>  0.69400\n","te juro que me estas hinchando las pelotas ==>  0.01859 100% recomendable ==>  0.51600\n","no me rompes el orto ==>  0.06346                       Volvimos satisfechos ==>  0.93699\n","esa situacion es imbancable ==>  0.04976                Comimos como diez personas ==>  0.88577\n","la verdad que no doy mas de vos ==>  0.26404            Vos sos lo mas ==>  0.57648\n","estoy harto de tu mala onda ==>  0.10659                me hizo reir un monton ==>  0.52237\n","sos un forro ==>  0.05210                               fue un placer conocerlos ==>  0.93403\n","para de mandarte cagadas asi ==>  0.02777               Con mucho gusto volveria ==>  0.91356\n","me estas cagando ??? ==>  0.02181                       Es una obra maestra !! ==>  0.88014\n"]}],"source":["X = ['Andate a cagar hdp', 'Chupenme la', 'Pelotudos del orto',\n","    'Chorros de mierda', 'Vos sos gil o que', 'tarado callate', \n","    'Me cago en la concha de tu hermana', 'La puta que lo pario',\n","    'la concha de la lora, te voy a matar', 'ni enpedo votaré para amalia granata', \n","    'esos chicos son tarados completos', 'la recalcada puta que lo pario',\n","    'hijo de mil putas', 'es una porkeria', 'que quilombo', 'Una verdadera choteada',\n","    'Vos decis pelotudeces todo el tiempo', 'vinimos al re pedo', 'ese pibe es muy cagon',\n","    'dale no seas rata', 'te juro que me estas hinchando las pelotas', 'no me rompes el orto', \n","    'esa situacion es imbancable', 'la verdad que no doy mas de vos', 'estoy harto de tu mala onda',\n","    'sos un forro', 'para de mandarte cagadas asi', 'me estas cagando ???']\n","\n","Y = ['Que piola che', 'Ese viaje es barbaro', 'Un chico muy copado!!',\n","    'Buenisima la atencion', 'La posta que estuvo espectacular la comida',\n","    'Es una ciudad muy buena onda', 'No te podes aburrir con ellos!',\n","     'Excelente, nada que decir', 'Reeeeee recomiendo esa pelicula',\n","    'Todo estuvo perfecto', 'El bife de lomo riquisimo', 'Me enamoré de esos paisajes',\n","    'Vamooo Argentina!!', 'Me quedé muy contento', 'El precio es honesto',\n","    'Ese hotel tenia todo para satisfechar', 'Muy bomba la pasamos', 'Los apoyo con todo mi ser',\n","    'confio totalmente en esa persona', 'Es el mejor dia de mi vida creo', '100% recomendable',\n","    'Volvimos satisfechos', 'Comimos como diez personas', 'Vos sos lo mas',\n","    'me hizo reir un monton', 'fue un placer conocerlos', 'Con mucho gusto volveria',\n","    'Es una obra maestra !!']\n","\n","print(' ')\n","print(' '.join([''] * 20) + 'MALOS' + ' '.join([''] * 60) + 'BUENOS')\n","print(' ')\n","for e,f in list(zip(X,Y)):\n","    toPrint1 = e + ' ==>  %.5f' % clf.predict(e)\n","    toPrint2 = f + ' ==>  %.5f' % clf.predict(f)\n","    print(toPrint1 + ' '.join([''] * (57-len(toPrint1))) + toPrint2)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lL_ODIajexxI"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"colab":{"provenance":[{"file_id":"https://github.com/aylliote/senti-py/blob/master/demo_classifier.ipynb","timestamp":1670918133118}]}},"nbformat":4,"nbformat_minor":0}